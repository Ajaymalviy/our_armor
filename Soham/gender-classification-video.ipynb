{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-16 13:10:42.207677: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-16 13:10:42.220019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-16 13:10:42.223567: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-16 13:10:42.234095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-16 13:10:43.041092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723794043.760111    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723794043.761661    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723794043.761831    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723794043.763073    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723794043.763255    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723794043.763397    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723794043.826086    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723794043.826282    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1723794043.826438    9485 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-16 13:10:43.826623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4232 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Using cache found in /home/shanks/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2024-8-4 Python-3.10.14 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723794048.720025    9583 service.cc:146] XLA service 0x76c358002900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723794048.720097    9583 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2024-08-16 13:10:48.745023: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-16 13:10:48.988168: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "Person 0: Gender prediction = 0.44, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 0: Gender prediction = 0.33, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 0: Gender prediction = 0.36, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 0: Gender prediction = 0.37, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723794051.358208    9583 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 0: Gender prediction = 0.38, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 0: Gender prediction = 0.49, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 0: Gender prediction = 0.41, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Person 0: Gender prediction = 0.41, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Person 0: Gender prediction = 0.40, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 0: Gender prediction = 0.45, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 0: Gender prediction = 0.36, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 0: Gender prediction = 0.51, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 0: Gender prediction = 0.51, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 0: Gender prediction = 0.53, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 0: Gender prediction = 0.50, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 1: Gender prediction = 0.48, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 1: Gender prediction = 0.65, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 1: Gender prediction = 0.50, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 1: Gender prediction = 0.39, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 1: Gender prediction = 0.41, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Person 1: Gender prediction = 0.38, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 1: Gender prediction = 0.43, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 1: Gender prediction = 0.42, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Person 1: Gender prediction = 0.34, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 1: Gender prediction = 0.38, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 1: Gender prediction = 0.48, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 1: Gender prediction = 0.50, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 1: Gender prediction = 0.35, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 1: Gender prediction = 0.43, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 1: Gender prediction = 0.42, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 2: Gender prediction = 0.17, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Person 2: Gender prediction = 0.28, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Person 2: Gender prediction = 0.24, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 2: Gender prediction = 0.36, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 2: Gender prediction = 0.49, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Person 2: Gender prediction = 0.29, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 2: Gender prediction = 0.21, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 2: Gender prediction = 0.12, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 2: Gender prediction = 0.10, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 2: Gender prediction = 0.10, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 2: Gender prediction = 0.39, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 2: Gender prediction = 0.24, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 2: Gender prediction = 0.18, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 2: Gender prediction = 0.09, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 2: Gender prediction = 0.32, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 3: Gender prediction = 0.24, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 3: Gender prediction = 0.16, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 3: Gender prediction = 0.47, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 3: Gender prediction = 0.39, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 3: Gender prediction = 0.56, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 4: Gender prediction = 0.42, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 4: Gender prediction = 0.30, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 4: Gender prediction = 0.28, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 4: Gender prediction = 0.51, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 4: Gender prediction = 0.52, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 4: Gender prediction = 0.42, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Person 4: Gender prediction = 0.38, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 4: Gender prediction = 0.60, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 4: Gender prediction = 0.26, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Person 4: Gender prediction = 0.42, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 5: Gender prediction = 0.17, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.30, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Person 5: Gender prediction = 0.25, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.47, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.45, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.36, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.21, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.44, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.81, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Person 5: Gender prediction = 0.57, Classified as = Female\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 5: Gender prediction = 0.13, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.18, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.18, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Person 5: Gender prediction = 0.11, Classified as = Male\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Person 5: Gender prediction = 0.32, Classified as = Male\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "from collections import deque\n",
    "\n",
    "# Load the gender classification model (fine-tuned MobileNetV2)\n",
    "model = load_model('Models/gender_classification_model_mobilenetv2.h5')\n",
    "\n",
    "# Load the YOLOv5 model for person detection\n",
    "model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Video file or webcam\n",
    "video_path = 0 # use 0 for webcam or provide path to video\n",
    "\n",
    "# Function to preprocess image for gender classification\n",
    "def preprocess_for_classification(image):\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = preprocess_input(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "# Function to calculate IoU (Intersection over Union) for two bounding boxes\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "\n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1 + w1, x2 + w2)\n",
    "    yi2 = min(y1 + h1, y2 + h2)\n",
    "\n",
    "    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "# Threaded frame capture class\n",
    "class FrameCapture(Thread):\n",
    "    def __init__(self, src, queue):\n",
    "        super().__init__()\n",
    "        self.cap = cv2.VideoCapture(src)\n",
    "        self.queue = queue\n",
    "        self.stopped = False\n",
    "\n",
    "    def run(self):\n",
    "        while not self.stopped:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                self.stop()\n",
    "                break\n",
    "            if self.queue.qsize() < 10:\n",
    "                self.queue.put(frame)\n",
    "\n",
    "    def stop(self):\n",
    "        self.stopped = True\n",
    "        self.cap.release()\n",
    "\n",
    "# Start frame capture thread\n",
    "frame_queue = Queue()\n",
    "capture_thread = FrameCapture(video_path, frame_queue)\n",
    "capture_thread.start()\n",
    "\n",
    "frame_count = 0\n",
    "tracking_data = {}\n",
    "next_person_id = 0\n",
    "\n",
    "# Process video frames\n",
    "while True:\n",
    "    if not frame_queue.empty():\n",
    "        frame = frame_queue.get()\n",
    "        frame_count += 1\n",
    "\n",
    "        if frame_count % 5 == 0:  # Perform detection every 5 frames\n",
    "            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = model_yolo(img_rgb)\n",
    "            new_detections = []\n",
    "\n",
    "            for *xyxy, conf, cls in results.xyxy[0].cpu().numpy():\n",
    "                if cls == 0 and conf > 0.5:  # Class 0 is 'person'\n",
    "                    x, y, w, h = int(xyxy[0]), int(xyxy[1]), int(xyxy[2]-xyxy[0]), int(xyxy[3]-xyxy[1])\n",
    "                    new_detections.append([x, y, w, h, conf])\n",
    "\n",
    "            updated_tracking_data = {}\n",
    "            for detection in new_detections:\n",
    "                x, y, w, h, confidence = detection\n",
    "                matched = False\n",
    "\n",
    "                for person_id, data in tracking_data.items():\n",
    "                    tracked_box = data['box']\n",
    "                    iou = calculate_iou(tracked_box, (x, y, w, h))\n",
    "\n",
    "                    if iou > 0.5:\n",
    "                        data['box'] = (x, y, w, h)\n",
    "                        data['frames_seen'] = frame_count\n",
    "                        updated_tracking_data[person_id] = data\n",
    "                        matched = True\n",
    "                        break\n",
    "\n",
    "                if not matched:\n",
    "                    updated_tracking_data[next_person_id] = {\n",
    "                        'box': (x, y, w, h),\n",
    "                        'gender_votes': deque(maxlen=15),  # Increased window for votes\n",
    "                        'fixed_gender': None,\n",
    "                        'frames_seen': frame_count\n",
    "                    }\n",
    "                    next_person_id += 1\n",
    "\n",
    "            tracking_data = updated_tracking_data\n",
    "\n",
    "        # Process each tracked person\n",
    "        for person_id, data in list(tracking_data.items()):\n",
    "            x, y, w, h = data['box']\n",
    "            person_img = frame[y:y+h, x:x+w]\n",
    "\n",
    "            if data['fixed_gender'] is None:\n",
    "                input_img = preprocess_for_classification(person_img)\n",
    "                gender_pred = model.predict(input_img)[0][0]\n",
    "                gender = 'Male' if gender_pred < 0.5 else 'Female'\n",
    "\n",
    "                # Debugging: Print the prediction confidence\n",
    "                print(f\"Person {person_id}: Gender prediction = {gender_pred:.2f}, Classified as = {gender}\")\n",
    "\n",
    "                data['gender_votes'].append(gender)\n",
    "\n",
    "                # Use the most frequent gender prediction after collecting enough votes\n",
    "                if len(data['gender_votes']) >= data['gender_votes'].maxlen:\n",
    "                    vote_count = np.array(data['gender_votes'])\n",
    "                    male_votes = np.sum(vote_count == 'Male')\n",
    "                    female_votes = np.sum(vote_count == 'Female')\n",
    "\n",
    "                    if male_votes > female_votes:\n",
    "                        data['fixed_gender'] = 'Male'\n",
    "                    elif female_votes > male_votes:\n",
    "                        data['fixed_gender'] = 'Female'\n",
    "                    else:\n",
    "                        data['fixed_gender'] = 'Uncertain'\n",
    "\n",
    "            else:\n",
    "                gender = data['fixed_gender']\n",
    "\n",
    "            color = (0, 255, 0) if gender == 'Male' else (0, 0, 255)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f'{gender} ({frame_count - data[\"frames_seen\"]} frames)', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            if frame_count - data['frames_seen'] > 150:\n",
    "                del tracking_data[person_id]\n",
    "\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "capture_thread.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipreqs\n",
      "  Downloading pipreqs-0.5.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting docopt==0.6.2 (from pipreqs)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ipython==8.12.3 (from pipreqs)\n",
      "  Downloading ipython-8.12.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting nbconvert<8.0.0,>=7.11.0 (from pipreqs)\n",
      "  Downloading nbconvert-7.16.4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting yarg==0.1.9 (from pipreqs)\n",
      "  Downloading yarg-0.1.9-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting backcall (from ipython==8.12.3->pipreqs)\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: decorator in /home/shanks/.local/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/shanks/.local/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/shanks/.local/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (0.1.7)\n",
      "Requirement already satisfied: pickleshare in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/shanks/.local/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/shanks/.local/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /home/shanks/.local/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /home/shanks/.local/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from ipython==8.12.3->pipreqs) (4.9.0)\n",
      "Requirement already satisfied: requests in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from yarg==0.1.9->pipreqs) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/shanks/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /home/shanks/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /home/shanks/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.1.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /home/shanks/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.8.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.9.2)\n",
      "Requirement already satisfied: packaging in /home/shanks/.local/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (24.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.2.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/shanks/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython==8.12.3->pipreqs) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/shanks/.local/lib/python3.10/site-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.2.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/shanks/.local/lib/python3.10/site-packages (from nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (8.6.2)\n",
      "Requirement already satisfied: fastjsonschema in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.19.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pexpect>4.3->ipython==8.12.3->pipreqs) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/shanks/.local/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.12.3->pipreqs) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/shanks/.local/lib/python3.10/site-packages (from beautifulsoup4->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/shanks/.local/lib/python3.10/site-packages (from requests->yarg==0.1.9->pipreqs) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/shanks/.local/lib/python3.10/site-packages (from requests->yarg==0.1.9->pipreqs) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/shanks/.local/lib/python3.10/site-packages (from requests->yarg==0.1.9->pipreqs) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from requests->yarg==0.1.9->pipreqs) (2024.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/shanks/.local/lib/python3.10/site-packages (from stack-data->ipython==8.12.3->pipreqs) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/shanks/.local/lib/python3.10/site-packages (from stack-data->ipython==8.12.3->pipreqs) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/shanks/.local/lib/python3.10/site-packages (from stack-data->ipython==8.12.3->pipreqs) (0.2.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/shanks/.local/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /home/shanks/.local/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/shanks/.local/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.4.1)\n",
      "Downloading pipreqs-0.5.0-py3-none-any.whl (33 kB)\n",
      "Downloading ipython-8.12.3-py3-none-any.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.3/798.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
      "Downloading nbconvert-7.16.4-py3-none-any.whl (257 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.4/257.4 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=168dbd3885192e77535fb3488f694dbb88251139a02389d8ba6d07b05bff1fee\n",
      "  Stored in directory: /home/shanks/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, backcall, yarg, ipython, nbconvert, pipreqs\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.25.0\n",
      "    Uninstalling ipython-8.25.0:\n",
      "      Successfully uninstalled ipython-8.25.0\n",
      "  Attempting uninstall: nbconvert\n",
      "    Found existing installation: nbconvert 7.10.0\n",
      "    Uninstalling nbconvert-7.10.0:\n",
      "      Successfully uninstalled nbconvert-7.10.0\n",
      "Successfully installed backcall-0.2.0 docopt-0.6.2 ipython-8.12.3 nbconvert-7.16.4 pipreqs-0.5.0 yarg-0.1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install pipreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Import named \"numpy\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"numpy\" was resolved to \"numpy:2.0.1\" package (https://pypi.org/project/numpy/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "WARNING: Import named \"pandas\" not found locally. Trying to resolve it at the PyPI server.\n",
      "WARNING: Import named \"pandas\" was resolved to \"pandas:2.2.2\" package (https://pypi.org/project/pandas/).\n",
      "Please, verify manually the final list of requirements.txt to avoid possible dependency confusions.\n",
      "INFO: Successfully saved requirements file in /home/shanks/data-science/CCTV-cam/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!pipreqs --scan-notebooks --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
